# TODO: Add more evaluation metrics, such as...

from pathlib import Path
from .one_row_keyboard_model import OneRowKeyboardModel, ModelEvaluator

# ① Create the model
model = OneRowKeyboardModel()

# ② Feed in the full training file generated by the processor
model.load_training_data("training_batch_full.txt")

# (Optional) Save it to avoid reloading next time
model.save_model("t9_big.json")

evaluator = ModelEvaluator(model)

results = evaluator.evaluate_accuracy("challenging_test.txt")

print(f"\nSentence complete match rate: {results['sentence_accuracy']:.3%}")
print(f"Word-by-word hit rate       : {results['word_accuracy']:.3%}")
print(f"Test samples                : {results['total_cases']:,}")
print(f"Exactly matched sentences   : {results['exact_matches']:,}")

# Look at a few specific examples
for r in results['detailed_results'][:5]:
    print("-" * 40)
    print("INPUT :", r['input'])
    print("GT    :", r['expected'])
    print("PRED  :", r['predicted'])
    print("OK?   :", "✓" if r['exact_match'] else "✗")

# Speed Evaluation
# Select 1,000 input sequences and run 5 rounds
inputs = [r['input'] for r in results['detailed_results'][:1000]]

speed = evaluator.evaluate_speed(inputs, num_runs=5)

print(f"\nAverage Time/Round   : {speed['avg_time_per_run']:.3f}s")
print(f"Decoding sequences per second : {speed['sequences_per_second']:.1f} seq/s")
